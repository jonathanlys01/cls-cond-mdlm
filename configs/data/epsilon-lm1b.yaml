train: epsilon-lm1b
valid: epsilon-lm1b
tokenizer_name_or_path: gpt2
cache_dir: ${scratch:}/data
wrap: True